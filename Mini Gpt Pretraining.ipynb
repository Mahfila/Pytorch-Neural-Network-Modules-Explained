{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9b86084-e120-4cb8-8e7f-6d4c737a7b6f",
   "metadata": {},
   "source": [
    "## V1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43614e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd05746-bb1b-48a9-8e35-2519032d99d3",
   "metadata": {},
   "source": [
    "### V1 - Introduction \n",
    "- Attention is all You need\n",
    "- Transformer\n",
    "- Transformer Decoder\n",
    "- Andrew Karpathy\n",
    "- GPT (Generative pre-trained transformer) - Decoder Only transformer\n",
    "- This is a very simplfied version, some steps, layers skipped\n",
    "- An introductory overview of ChatGPT, covering how large language models are trained. This tutorial is motivated by a video by Andrew Karpathy\n",
    "  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e4c40d-2e18-4a32-a258-4c9cb76be0e8",
   "metadata": {},
   "source": [
    "### V2 - Tokenization\n",
    "Tokenization is the process of converting a sequence of characters into a sequence of tokens. In this script, the given text is read from an input.txt file. Every unique character in this text is treated as a token, leading to a vocabulary of unique characters. The script then provides utilities (encode and decode functions) to convert strings to their tokenized representations and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fea6936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!$&',-.3:;? \n",
      "\n",
      "65\n",
      "[7, 8, 8, 63, 19, 7, 4, 17, 4]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "chars = sorted(list(set(text)))\n",
    "chars = list(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!$&',-.3:;? \\n\")\n",
    "vocab_size = len(chars)\n",
    "all_chars = ''.join(chars)\n",
    "print(all_chars)\n",
    "print(vocab_size)\n",
    "stoi = {ch:i for i, ch in enumerate(chars) }\n",
    "itos = {i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join(itos[i] for i in l)\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))\n",
    "\n",
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device),y.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433722a3-6dd7-441e-a496-6fdcd59e53ee",
   "metadata": {},
   "source": [
    "### V3 - Parameters, Embeeding and Positional Encoding\n",
    "Parameter - This section merely lists out various hyperparameters and parameters used in the training and model. This includes things like batch size, the maximum number of iterations, the learning rate, and so on.\n",
    "\n",
    "Embeddings are a way of representing categorical data, like words or characters, as continuous vectors. In this script, each character is embedded into a continuous vector space using an embedding layer.\n",
    "\n",
    "Positional encodings are added to give the model information about the relative position of tokens in a sequence. They're crucial for models like Transformers, which otherwise wouldn't have any idea about the order of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01475e05-fbc4-4f25-ba22-71153e155e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "block_size = 4\n",
    "max_iters = 10000\n",
    "learning_rate = 3e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps'\n",
    "eval_interval = 1\n",
    "n_embd = 4\n",
    "dropout = 0.2\n",
    "\n",
    "token_embedding_table = nn.Embedding(vocab_size, n_embd).to(device)\n",
    "positional_embedding_table = nn.Embedding(block_size, n_embd).to(device)\n",
    "xb, yb = get_batch('train')\n",
    "B, T = xb.shape\n",
    "tok_emb = token_embedding_table(xb.to(device))\n",
    "pos_emb = positional_embedding_table(torch.arange(T, device=xb.device))\n",
    "x = tok_emb + pos_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "300c2876-c568-45e7-a707-5241bed07889",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_layer = nn.Dropout(dropout).to(device)\n",
    "tril = torch.tril(torch.ones(block_size, block_size)).to(device)\n",
    "ln_f = nn.LayerNorm(n_embd).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312e0bfd-8f6d-42f6-b1da-6fd944e7adb4",
   "metadata": {},
   "source": [
    "### V4 - Multihead Attention Layer 1 Head 1\n",
    "The Multihead Attention mechanism allows the model to focus on different parts of the input sequence when producing an output sequence. The mechanism works by producing multiple sets (or \"heads\") of key, query, and value projections, then combining them. This script appears to define a transformer with 2 layers, and each layer has 2 heads of attention.\n",
    "\n",
    "Multihead Attention Layer 1 Head 1 & Head 2\n",
    "These are the first and second heads of the multihead attention mechanism in the first layer. The keys, queries, and values are computed using linear projections of the input, and then the attention weights are calculated and used to produce the output of the attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "570220a3-3197-490b-ba78-fccf07b311ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardcoded keys, queries, and values for 2 layers with 2 heads each\n",
    "key_1_1 = nn.Linear(n_embd, n_embd // 2, bias=False).to(device)  # Neuron\n",
    "query_1_1 = nn.Linear(n_embd, n_embd // 2, bias=False).to(device)\n",
    "value_1_1 = nn.Linear(n_embd, n_embd // 2, bias=False).to(device)\n",
    "k1 = key_1_1(x)\n",
    "q1 = query_1_1(x)\n",
    "v1 = value_1_1(x)\n",
    "wei1 = (q1 @ k1.transpose(-2, -1)) * (n_embd**-0.5)\n",
    "wei1 = F.softmax(wei1.masked_fill(tril[:T, :T] == 0, float('-inf')), dim=-1)\n",
    "out1 = wei1 @ v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8132988-5d94-4477-a25d-a252b03b8b95",
   "metadata": {},
   "source": [
    "### V4 - Multihead Attention Layer 1 Head 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbc5f270-e5af-4411-b4b9-5fd50b9c9747",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_1_2 = nn.Linear(n_embd, n_embd // 2, bias=False).to(device)\n",
    "query_1_2 = nn.Linear(n_embd, n_embd // 2, bias=False).to(device)\n",
    "value_1_2 = nn.Linear(n_embd, n_embd // 2, bias=False).to(device)\n",
    "k2 = key_1_2(x)\n",
    "q2 = query_1_2(x)\n",
    "v2 = value_1_2(x)\n",
    "wei2 = (q2 @ k2.transpose(-2, -1)) * (n_embd**-0.5)\n",
    "wei2 = F.softmax(wei2.masked_fill(tril[:T, :T] == 0, float('-inf')), dim=-1)\n",
    "out2 = wei2 @ v2\n",
    "out = torch.cat([out1, out2], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a183add-e711-4513-8c3f-f9e2feaaa21b",
   "metadata": {},
   "source": [
    "### V5- Layer 1 Add and Norm  - Feedforward - Add and Norm\n",
    "After the attention outputs for each head are computed, they are concatenated and then passed through a feedforward network. The Add and Norm steps involve adding the original input to the output of the attention or feedforward networks (a form of residual connection), and then normalizing the result. This helps in stabilizing the activations and aids in training deeper models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6875fed8-1541-44c8-a013-883bf27b04cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_1 = nn.Linear(n_embd, n_embd).to(device)\n",
    "\n",
    "ffwd_1_fc1 = nn.Linear(n_embd, 4 * n_embd).to(device)\n",
    "ffwd_1_relu = nn.ReLU().to(device)\n",
    "ffwd_1_fc2 = nn.Linear(4 * n_embd, n_embd).to(device)\n",
    "ffwd_1_dropout = nn.Dropout(dropout).to(device)\n",
    "\n",
    "ln_1_1 = nn.LayerNorm(n_embd).to(device)\n",
    "ln_1_2 = nn.LayerNorm(n_embd).to(device)\n",
    "\n",
    "x = x + dropout_layer(proj_1(out))\n",
    "x = ln_1_1(x)\n",
    "\n",
    "x_ffwd_1 = ffwd_1_fc1(x)\n",
    "x_ffwd_1 = ffwd_1_relu(x_ffwd_1)\n",
    "x_ffwd_1 = ffwd_1_fc2(x_ffwd_1)\n",
    "x_ffwd_1 = ffwd_1_dropout(x_ffwd_1)\n",
    "\n",
    "x = x + x_ffwd_1\n",
    "x = ln_1_2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf64a2a-ab76-4f44-8165-9264131e284c",
   "metadata": {},
   "source": [
    "### V6 - Multihead Attention Layer 2 Head 1\n",
    "Similar to the first layer, this defines the multihead attention mechanism for the second transformer layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0420c57a-918a-423a-bb99-da7912f036cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_2_1 = nn.Linear(n_embd, n_embd // 2, bias=False).to(device)\n",
    "query_2_1 = nn.Linear(n_embd, n_embd // 2, bias=False).to(device)\n",
    "value_2_1 = nn.Linear(n_embd, n_embd // 2, bias=False).to(device)\n",
    "\n",
    "k1 = key_2_1(x)\n",
    "q1 = query_2_1(x)\n",
    "v1 = value_2_1(x)\n",
    "wei1 = (q1 @ k1.transpose(-2, -1)) * (n_embd**-0.5)\n",
    "wei1 = F.softmax(wei1.masked_fill(tril[:T, :T] == 0, float('-inf')), dim=-1)\n",
    "out1 = wei1 @ v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6dfc21-c0ad-4607-97ec-8488d34b80df",
   "metadata": {},
   "source": [
    "### V6 -Multihead Attention Layer 2 Head 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b972e5e5-6273-43a9-8396-ad67dd20c748",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_2_2 = nn.Linear(n_embd, n_embd // 2, bias=False).to(device)\n",
    "query_2_2 = nn.Linear(n_embd, n_embd // 2, bias=False).to(device)\n",
    "value_2_2 = nn.Linear(n_embd, n_embd // 2, bias=False).to(device)\n",
    "\n",
    "k2 = key_2_2(x)\n",
    "q2 = query_2_2(x)\n",
    "v2 = value_2_2(x)\n",
    "wei2 = (q2 @ k2.transpose(-2, -1)) * (n_embd**-0.5)\n",
    "wei2 = F.softmax(wei2.masked_fill(tril[:T, :T] == 0, float('-inf')), dim=-1)\n",
    "out2 = wei2 @ v2\n",
    "out = torch.cat([out1, out2], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353b23b0-7fc2-4266-b96c-2d1e52aff350",
   "metadata": {},
   "source": [
    "### v7 - Layer 2 Add and Norm - Feedforward - Add and Norm\n",
    "Just as in the first layer, after the attention outputs for each head in the second layer are computed, they're concatenated, passed through a feedforward network, and then subjected to the add and normalize operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ce98145-a23e-48fc-a379-b0cd685e5278",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_2 = nn.Linear(n_embd, n_embd).to(device)\n",
    "\n",
    "ffwd_2_fc1 = nn.Linear(n_embd, 4 * n_embd).to(device)\n",
    "ffwd_2_relu = nn.ReLU().to(device)\n",
    "ffwd_2_fc2 = nn.Linear(4 * n_embd, n_embd).to(device)\n",
    "ffwd_2_dropout = nn.Dropout(dropout).to(device)\n",
    "\n",
    "ln_2_1 = nn.LayerNorm(n_embd).to(device)\n",
    "ln_2_2 = nn.LayerNorm(n_embd).to(device)\n",
    "\n",
    "x = x + dropout_layer(proj_2(out))\n",
    "x = ln_2_1(x)\n",
    "x_ffwd_2 = ffwd_2_fc1(x)\n",
    "x_ffwd_2 = ffwd_2_relu(x_ffwd_2)\n",
    "x_ffwd_2 = ffwd_2_fc2(x_ffwd_2)\n",
    "x_ffwd_2 = ffwd_2_dropout(x_ffwd_2)\n",
    "\n",
    "\n",
    "x = x + x_ffwd_2\n",
    "x = ln_2_2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec8523c-2713-43ff-8fcb-aa7af21f506c",
   "metadata": {},
   "source": [
    "### v8 - Final Feedforward\n",
    "The output from the last transformer layer is passed through a linear layer (often termed as the head of the model) to produce logits for each token in the vocabulary. These logits can be used to predict the next token in a sequence, making this a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef368b10-1402-4a63-aa38-06c88e0d3882",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_head = nn.Linear(n_embd, vocab_size).to(device)\n",
    "logits = lm_head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd66d890-7cf9-4570-9217-65440cf7e9f7",
   "metadata": {},
   "source": [
    "### v9 - Loss and backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4b340f1-0262-49a2-b471-52d130a71f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.3645, device='mps:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW([\n",
    "    *token_embedding_table.parameters(),\n",
    "    *positional_embedding_table.parameters(),\n",
    "    *key_1_1.parameters(), *key_1_2.parameters(), *key_2_1.parameters(), *key_2_2.parameters(),\n",
    "    *query_1_1.parameters(), *query_1_2.parameters(), *query_2_1.parameters(), *query_2_2.parameters(),\n",
    "    *value_1_1.parameters(), *value_1_2.parameters(), *value_2_1.parameters(), *value_2_2.parameters(),\n",
    "    *proj_1.parameters(), *proj_2.parameters(),\n",
    "    *ffwd_1_fc1.parameters(), *ffwd_1_fc2.parameters(),\n",
    "    *ffwd_2_fc1.parameters(), *ffwd_2_fc2.parameters(),\n",
    "    *ln_1_1.parameters(), *ln_1_2.parameters(), *ln_2_1.parameters(), *ln_2_2.parameters(),\n",
    "    *ln_f.parameters(),\n",
    "    *lm_head.parameters()\n",
    "], lr=learning_rate)\n",
    "\n",
    "loss = F.cross_entropy(logits.view(-1, vocab_size), yb.view(-1).to(device))\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
